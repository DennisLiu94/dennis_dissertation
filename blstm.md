	在本论文的研究中，作者以一个张姝，胡新辰等实现的BLSTM网络作为基线模型并在此基础上进行改进实验。本章中主要介绍基线模型和作者的实现。

	LSTM模型是RNN的改进版本，通过遗忘门等机制的加入尝试解决RNN在训练中梯度耗散和爆炸的问题。
	胡新辰等人的模型将实体关系分类模型当做一个19类分类问题处理，即不区分关系种类和关系方向这两个分类问题。

	模型的工作流程如下：
		1.全部输入特征做embedding
		2.通过一个双向LSTM层获得句子表示
		3.在句子表示上做double max pooling
		4.将pooling结果，目标实体位置对应step的lstm输出，目标实体位置对应的原始语句特征embedding作为中间特征，合并后进入一层隐层。
		5.隐层输出进入softmaxt分类器。

	上述模型的输入特征可以分为词向量，postag，ner，依存特征，位置特征5类。
	第一类词特征指通过词向量替换语句中词语得到的向量表示。根据张姝等人的论文，作者在这里选择表现较好的glove100作为全文实验的word embedding。
	第二类postag特征指句子中每个词的postag类别。
	第三类ner特征指句子中每个词语的ner类别。
	第四类依存特征分为三个部分。首先是相对依存关系：即语句中每个词语在依存句法树中和两个目标实体的相对关系，有子节点，父节点和无关三类。其实是依存关系类别，该特征通过著名的stanford parser得到，具体类别也依照stanford parser给出的规则不做处理。最后还有一种。。。。。。
	第五类相对位置特征指句子中每个词相对两个目标实体的相对位置，这一特征首次是在曾道建等的论文中提出。

	以上各类特征除第一类外，都是离散的类别特征，在输入神经网络前需经过一步随机embedding处理。

	LSTM网络根据句子输入方向可以分为正向和反向两种，这里作者使用双向LSTM来最大限度的获得句子中的信息。
	胡新辰根据目标实体之间的内容对判别目标实体关系更重要这一任务特点，设计了double max pooling这一方法。其和普通max pooling的不同之处在于进行pooling运算前先将句子的向量表示按照实体位置分段，在图示的两段上分别进行max pooling。这样两目标实体之间的部分在pooling结果上具有更大的权重。
	值得注意的是，由于该模型同样会取出目标实体对应位置的LSTM输出，故在pooling过程中忽略目标实体对应的位置。

	下面接续的隐层起到融合不同特征的作用，最终进入一个softmax分类器对实体关系类别进行判断。

	模型的数据流见图。

	这里需要说明，该模型大量是用了胡新辰硕士论文中介绍的trick。主要包括网络权重初始化和dropout。

	具体地，作者使用X。。。方法进行权值初始化，以避免神经网络在初始阶段的训练困难。在训练阶段引入dropout来对抗过拟合，期待模型学到更多的独立特征而不是联合特征。
	在训练方面，作者使用minibatch的训练方式结合AdaDelta自适应学习率的方法，以期取得较快的收敛速度。

	作者实现的模型的具体参数见下表。


	最终，作者实现的LSTM模型的表现以及和其它模型的对比见下表。

	可以看到，无论是有特征还是无特征模型，都都达到了state of art。这说明使用LSTM对自然语言语句建立句子表示是一种较恰当的方法。


	结论：作者根据张姝等人的论文，实现了一个使用LSTM模型建立句子表示的基线系统，达到了该任务上的state of art。LSTM模型对于实体关系分类任务而言，是一个较恰当的建立句子表示的方法。
