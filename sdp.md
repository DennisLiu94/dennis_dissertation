	实体关系分类任务通常被视作一个分类任务，在过去的研究中研究者们提出了各种分类特征。如在2010年的评测任务当时表现最好的系统的作者设计了Lexical（词法），Dependency（依存关系）等8组共45个特征。[2]更早的研究表明，在判断实体关系的任务中，句法特征的加入对于提升系统表现是非常重要的。事实上，大量有效信息集中在两个目标实体的最短依存路径上。[3]
	在NLP技术进入深度学习时代后，很多关于实体关系分类任务的研究中，研究者们都把整个句子的原始单词序列或者整个句子的句法树作为神经网络模型的系统输。如国内学者曾道建等的CDNN模型[4],国外学者C´ıcero Nogueira dos Santos等人的CR-CNN模型等。这种做法在需要判断关系的两个实体距离较远时，会收到较多无关信息的影响而表现下降。[1]
	在本章中，作者希望通过句法结构信息判断句子中哪些子结构和目标实体之间的关系相关而哪些不相关。
	两个词之间的最短依存路径是指两个词节点在依存句法树中的最短路径。一般而言，这一路径包含两个词节点在依存树上的最近公共祖先。
	在这里，称以两目标实体最近公共祖先为根节点的子树为目标子树，称从目标实体1到目标实体2的最短路径为目标最短依存路径。
	作者暂时假设两目标实体之间的关系信息蕴含在目标子树中；不在目标子树中的句子结构只将两个实体所在的目标子树作为整体行使句法作用，因此不会影响子树内部的语义。当然这一假设并不严格成立，后续的分析会说明这一点。
	
	之前的研究表明，在实体关系分类任务中，更需要关注的是连接两个目标实体的句子结构和两个实体本身。根据这个原则，作者在这里提出了两种预处理方法对句子内容进行不同程度的剪枝以达到选择出蕴含和目标相关语义信息较多的句子内容而过滤掉大量无关内容的目的。
	方法一：只保留目标最短依存路径上的词节点，将这些词节点按照依存树中目标实体一指向目标实体二的路径顺序排列。这一方法的出发点在于最大限度地过滤无关内容。此外，这种方法的另外一个优点在于可以形成从目标实体一指向目标实体二的语义链，这种结构在词序中即蕴含了依存关系信息，便于神经网络模型学习恰当的表示。
	方法二：在保留目标最短依存路径上的词节点的基础上添加两个目标实体的子树，将这些词节点按照原句子中的语序排列。和方法一相比，方法二保留了更多的目标实体语境信息。方法二的缺点在与破坏了方法一中自然形成的语义链结构，词序列中相互依存关系不明。此外，目标实体可能引入和任务无关的子树，引入了更多噪声。

	除是否保留语料外，还可以选择是否在输入中加入依存关系类别这一特征。

	按照上述两种方法和是否引入依存关系特征，作者进行了语料处理并得到四种模型输入。原始语料最大句长是95，经过方法一处理的最大句长是8，经过方法二处理的最大句长是13.分别使用第二章中介绍的BLSTM网络进行训练，得到如下的实验结果。


	观察实验结果，可以看到在不加入依存关系类别这一特征的情况下，方法二略优于方法一。在加入这一特征后，方法二差与方法一。
	根据这一实验结果，作者推测可能的解释是目标实体的子树确实包含了一些有用语义信息，因此在不加入特征的情况下略优于方法一。但在加入依存关系类别后，更明确的依存类别标签中的信息覆盖了目标实体的语义信息。此外，由于方法二的结构破坏了方法一中体现的依存关系，导致词语之间依存关系不明，这可能是导致模型表现劣化的重要原因。

	值得一提的是，由于句子长度变为原始语料的十分之一，在K40显卡上的训练速度从1.5分钟每轮加速到0.1分钟每轮，显存需求和从2000M降低到500M的水平。

	
	上述实验表明，目标依存路径是尽量保留目标语义信息条件下，尽量过滤无关句子内容的较好选择，其还具有良好的依存关系结构。在方法一的基础上，作者尝试进行了更深层的神经网络模型试验。
	在下面的实验中，作者将原本的神经网络模型加深到三层，如图，得到如下的结果。
	

	和单层网络相比，未加特征的情况下，深层网络略有提升。在加入特征的情况下，两者区别不大。
	作者认为这里的原因可能更深的神经网络学习的句子表示更加抽象，跨越的句长也越大，因此在未加特征的句子上表现有所提升。而在加入依存关系的句子中，该种方法已起不到太多作用。可能的原因是依存关系类别本身就是一种抽象特征；另一种可能原因是在已经经过缩短的句子上，在加入依存类别标签的情况下，单层BLSTM已经足够覆盖较多的单词。

	结论：根据上述实验，我们可以得出以下结论。从依存句法的角度出发，只保留目标最短依存路径，可以再保留主要信息的情况下较大程度地过滤掉无用信息；依存关系类别包含了暗示实体关系类别的重要信息，在加入此特征的情况下无需再在目标最短依存路径本身的信息上在做过多挖掘。
